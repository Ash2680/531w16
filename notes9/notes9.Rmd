---
title: "9. Introduction to partially observed Markov process models"
author: "Edward Ionides"
date: "2/11/2016"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_depth: 2
    number_sections: true
    pandoc_args: [
      "--number-offset=9"
    ]
csl: ecology.csl
---


\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\loglik{\ell}
\newcommand\R{\mathbb{R}}
\newcommand\data[1]{#1^*}
\newcommand\params{\, ; \,}
\newcommand\transpose{\scriptsize{T}}
\newcommand\eqspace{\quad\quad\quad}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}
\newcommand\profileloglik[1]{\ell^\mathrm{profile}_#1}
\newcommand\ar{\phi}
\newcommand\ma{\psi}
\newcommand\AR{\Phi}
\newcommand\MA{\Psi}
\newcommand\ev{u}
\newcommand\given{{\, | \,}}
\newcommand\matA{\mathbb{A}}
\newcommand\matB{\mathbb{B}}
\newcommand\covmatX{\mathbb{U}}
\newcommand\covmatY{\mathbb{V}}



Licensed under the Creative Commons attribution-noncommercial license, http://creativecommons.org/licenses/by-nc/3.0/.
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](cc-by-nc.png)

```{r knitr-opts,include=FALSE,cache=FALSE,purl=FALSE}
library(pomp)
library(knitr)
prefix <- "intro"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  cache_extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
)

set.seed(2050320976)
```
```{r opts,include=FALSE,cache=FALSE}
options(
  keep.source=TRUE,
  encoding="UTF-8"
)
```

-------------------

------------------

<big><big><big>Objectives</big></big></big>

* Develop a framework for thinking of models consisting of a stochastic dynamic system observed with noise.

* In the linear Gaussian case, develop matrix operations to find an exact and computationally fast algorithm for the likelihood function. This algorithm is called the **Kalman filter**.

* Understand how the Kalman filter is used to compute the likelihood for ARMA models.

* See how the Kalman filter also facilitates forecasting and estimation of the state of the unobserved process.

* Start to investigate the general nonlinear filtering equations.

<br>

----------------------

---------------

## Partially observed Markov processes (POMP) models

* Uncertainty and variability are ubiquitous features of processes in the biological and social sciences. A physical system obeying Newton's laws is fully predictable, but complex systems are in practice not perfectly predictable---we can only forecast weather reliably in the near future.

* Basic time series model of deterministic trend plus colored noise imply perfect reproducibility and (so far as the deterministic trend model can be extrapolated) forecasts that remain acurate far into the future.

* To model variability and unpredictability in low frequency components, we may wish to specify a random process model for how the system evolves. We could call this a "stochastic trend" approach, though that is an oxymoron since we've defined trend to be expected value.

* As in the deterministic signal plus noise model, we will model the observations as random variables conditional on the trajectory of the latent process.

* The model for how the latent Markov process evolves is therefore called a **latent process model** or a **hidden process model** to acknowledge that we are modeling a process that we suppose cannot directly be observed. It is also sometimes called a **state process** since we may think of the latent process as representing the indirectly measured state of some system.

* A standard class of latent process models is characterized by the requirement that the future evolution of the system depends only on the current state, plus randomness introduced in future. A model of this type is called a **Markov chain**, or, if the process is defined in continuous time rather than just at discrete time points, a **Markov process**. We will use the term Markov process for both discrete and continous time.

* **Partial observations** here mean either or both of (i) measurement noise; (ii) entirely unmeasured latent variables. Both these features are present in many systems.

* To specify a **partially observed Markov process** (POMP) model, we will need to specify a latent Markov process model. We must also then specify how the **observation process model** is supposed to generate the data given the latent process.

* In a POMP model, the latent Markov process model can also be called the **state process model** or the **hidden process model**. 

* Often, much of the scientific interest is in understanding what models for the behavior of this latent process are consistent with the data. 

* A good model for the underlying, but imperfectly observed, dynamics of a system can also lead to a skillful forecast.

* We are going to introduce a general framework for specifying POMP models. This generality will give us the flexibility to develop models and methods appropriate to a range of applications.

<br>

-------

-------

### Discrete time Markov processes

* A time series model $X_{0:N}$ is a **Markov process** model if the conditional densities satisfy the **Markov property** that, for all $n\in 1:N$. 

<br>

[MP1] $\eqspace f_{X_n|X_{1:n-1}}(x_n\given x_{1:n-1}) = f_{X_n|X_{n-1}}(x_n\given x_{n-1})$,

<br>

* We suppose that the random process $X_n$ occurs at time $t_n$ for $n\in 0:N$, so the discrete time process corresponds to time points in continuous time. 

* We have **initialized** the Markov process model at a time $t_0$, although we will suppose that data are collected only at times $t_{1:N}$.

    + The initialization model could be deterministic (a fixed value) or a random variable. 

    + Formally, a fixed initial value is a special case of a discrete distribution having a point mass with probability one at the fixed value. Therefore, fixed initial values are covered in our framework since we use probability density functions to describe both discrete and continuous probability distributions.

    + Mathematically, a probability mass function (for discrete distributions) is a probability density with respect to a [counting measure](https://en.wikipedia.org/wiki/Counting_measure). We don't have to get sidetracked on to that topic, but it is worth noting that there is a proper mathematical justification for treating a probability mass function as a type of probability density function.

    + It is not important whether to adopt the convention that the Markov process model is intialized at time $t_1$ or at some previous time $t_0$. Here, we follow the choice to use $t_0$.

* The probability density function $f_{X_n|X_{n-1}}(x_n\given x_{n-1})$ is called the **one-step transition density** of the Markov process.

* In words, the Markov property says that the next step taken by a Markov process follows the one-step transition density based on the current state, whatever the previous history of the process.

* For a POMP model, the full joint distribution of the latent process is entirely specified by the one-step transition densities, as we will show below. Therefore, we also call $f_{X_n|X_{n-1}}(x_n\given x_{n-1})$ the **process model**.



<br>

-----

-----

### Question: Write the joint distribution in terms of the one-step transition densities

* Use [MP1] to derive an expression for the joint distribution of a Markov process as a product of the one-step transition densities,

<br>

[MP2] $\eqspace f_{X_{0:N}}(x_{0:N}) = f_{X_0}(x_0)\prod_{n=1}^N f_{X_n|X_{n-1}}(x_n\given x_{n-1})$.

<br>

------

------

### Question: Show that a causal Gaussian AR(1) process is a Markov process.

<br>

-----

-----

### Time homogeneous transitions and stationarity

* In general, the one step transition probability density in a POMP model can depend on $n$. 

* A latent process model $X_{0:N}$ is **time-homogeneous** if the  one step transition probability density does not depend on $n$, i.e., if there is a conditional density $f(y\given x)$ such that, for all $n\in 1:N$,
$$  f_{X_n|X_{n-1}}(x_n\given x_{n-1})= f(x_n\given x_{n-1}).$$

* If $X_{0:N}$ is stationary then it is time homogeneous. 

    + Why? Go back to the definitions and show this from first principles. 

* Time homogeneity does not necessarily imply stationarity. 

    + Find a counter-example.

    + What has to be added to time homogeneity to get stationarity?

<br>

--------

--------


### The measurement model

* We model the observation process random variables $Y_{1:N}$.

* For state space models, we will generally write the data as $\data{y_{1:N}}$.

* We model the measurement at time $t_n$ to depend only on the value of the latent process at time $t_n$, conditionally independent of all other latent process and observation process variables. Formally, this assumption is,

<br>

[MP3] $\eqspace f_{Y_n|X_{0:N},Y_{1:n-1},Y_{n+1:N}}(y_n\given x_{0:N},y_{1:n-1},y_{n+1:N}) = f_{Y_n|X_n}(y_n\given x_{n})$.

<br>

* We call $f_{Y_n|X_n}(y_n\given x_{n})$ the **measurement model**.

* In general, the measurement model can depend on $n$. 

* The measurement model is **time-homogeneous** if there is a conditional probability density function $g(y\given x)$ such that, for all $n\in 1:N$,
$$ f_{Y_n|X_n}(y_n\given x_{n})= g(y_n\given x_n).$$

<br>

--------

--------

## Four basic calculations for working with POMP models

* Many time series models in science, engineering and industry can be written as POMP models.

* A reason that POMP models form a useful tool for statistical work is that there are convenient recursive formulas to carry out following four basic calculations.

<br>

------

-------

### Prediction

* One-step prediction of the latent process at time $t_{n+1}$ given data up to time $t_n$ involves finding
$$ f_{X_{n+1}|Y_{1:n}}(x_{n+1}\given \data{y_{1:n}}).$$

* We may want to carry out prediction (also called forecasting) more than one time step ahead. However, unless specified otherwise, the prediction calculation will be one-step prediction. 

* One-step prediction turns out to be closely related to computing the likelihood function, and therefore central to statistical inference.

* We have required our prediction to be a conditional probability density, not a point estimate. In the context of forecasting, this is called a **probabilistic forecast**, and has advantages over a point estimate forecast. What are they? Are there any disadvantages to probabilistic forecasting?

<br>

------

-------

### Filtering

* The filtering calculation at time $t_n$ is to find the conditional distribution of the latent process $X_n$ given currently available data, $\data{y_{1:n}}$.

* Filtering therefore involves calculating
$$f_{X_{n}|Y_{1:n}}(x_n\given \data{y_{1:n}}).$$



<br>

------

-------

### Smoothing

* In the context of a POMP model, smoothing involves finding the conditional distribution of $X_n$ given all the data, $\data{y_{1:N}}$.

* So, the smoothing calculation is
$$f_{X_{n}|Y_{1:N}}(x_n\given \data{y_{1:N}}).$$

<br>

------

-------

### The likelihood

* The model may depend on a parameter vector $\theta$.

* Since we haven't explicitly written this dependence above, the likelihood calculation is to evaluate the joint density of $Y_{1:N}$ at the data,
$$f_{Y_{1:N}}(\data{y_{1:N}}).$$

* If we can compute this for any value of $\theta$, we can perform numerical optimization to get a maximum likelihood estimate, compute profile likelihood confidence intervals, carry out likelihood ratio tests, and make AIC comparisons.


<br>

------

-------

### The prediction and filtering formulas

* One-step prediction of the latent process at time $t_{n}$ given data up to time $t_{n-1}$ can be computed in terms of the filtering problem at time $t_{n-1}$, via the **prediction formula** for $n\in 1:N$,

<br>

[MP4] $\eqspace \displaystyle f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})$

<br>

$\eqspace \eqspace \displaystyle = \int f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}\given \data{y_{1:n-1}}) 
f_{X_{n}|X_{n-1}}(x_{n}\given x_{n-1})\, dx_{n-1}$.

<br>

* To make this formula work for $n=1$, we need the convention that $1:k$ is the empty set when $k=0$. Conditioning on an empty collection of random variables is the same as not conditioning at all! In this case, we have by definition that
$$ f_{X_{0}|Y_{1:0}}(x_{0}\given \data{y_{1:0}}) = f_{X_0}(x_0).$$
In other words, the filtering calcuation at time $t_0$ is the initial density for the latent process. This makes sense, since at time $t_0$ we have no data to condition on.

* To see why the prediction formula is true, we can view it is an application of a general identity for joint continuous random variables $X$, $Y$, $Z$,
$$ f_{X|Y}(x\given y)= \int f_{XZ|Y}(x,z\given y)\, dz,$$
which is a condition form of the basic identity that integrating out a joint distribution gives a marginal distribution.


--------

-------

* Filtering at time $t_n$ can be computed by combining the new information in the datapoint $\data{y_{n}}$ with the calculation of the one-step prediction of the latent process at time $t_{n}$ given data up to time $t_{n-1}$. This is carried out via the **filtering formula** for $n\in 1:N$,

<br>

[MP5] $\eqspace \displaystyle f_{X_{n}|Y_{1:n}}(x_{n}\given \data{y_{1:n}})
= \frac{
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})\, 
f_{Y_n|X_n}(\data{y_n}\given x_n)
}{
f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})
}$.

<br>


* The denominator in the filtering formula [MP5] is the **conditional likelihood** of $\data{y_{n}}$ given  $\data{y_{1:n-1}}$. It can be computed in terms of the one-step prediction density, via the **conditional likelihood formula**,

<br>

[MP6] $\eqspace \displaystyle f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})
= 
\int 
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})\, f_{Y_n|X_n}(\data{y_n}\given x_n)\, dx_n$.

<br>

* To make this formula work for $n=1$, we again take advantage of the convention that $1:k$ is the empty set when $k=0$. 

* To see why the filtering formula is true, we can apply the identity  for joint continuous random variables $X$, $Y$, $Z$,
$$f_{X|YZ}(x\given y,z)= \frac{ f_{Y|XZ}(y\given x,z)\, f_{X|Z}(x\given z)}{f_{Y|Z}(y\given z)}.$$
This is a conditional form of the Bayes formula, which in turn is closely related to a conditional form of the basic definition of the conditional probability density function,
$$f_{X|YZ}(x\given y,z)= \frac{f_{XY|Z}(x,y\given z)}{f_{Y|Z}(y\given z)}.$$

<br>

--------

-------

* The prediction and filtering formulas are **recursive**. If they can be computed for time $t_n$ then they provide the foundation for the following computation at time $t_{n+1}$.

<br>

-----

----

### Question: Give a detailed derivation of [MP4], [MP5] and [MP6], being careful to note when you use the Markov property [MP1].

------

-------

### Computation of the likelihood

* The likelihood of the entire dataset, $\data{y_{1:N}}$ can be found from [MP6], using the identity

<br>

[MP7] $\eqspace\displaystyle f_{Y_{1:N}}(\data{y_{1:N}})
= \prod_{n=1}^N  f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})$.

<br>

* Yet again, this formula [MP7] requires the convention that $1:k$ is the empty set when $k=0$, so the first term in the product is
$$f_{Y_{1}|Y_{1:0}}(\data{y_1}\given \data{y_{1:0}}) = 
f_{Y_{1}}(\data{y_1}).$$

* If our model has an unknown parameter $\theta$, the likelihood identity [MP7] lets us evaluate the log likelihood function,
$$\loglik(\theta)=\log f_{Y_{1:N}}(\data{y_{1:N}}\params\theta).$$


<br>

------

-------

### The smoothing formulas

* Smoothing is less fundamental for likelihood-based inference than filtering and one-step prediction. 

* Nevertheless, sometimes we want to compute the smoothing density, so let's obtain some necessary formulas.

* The filtering and prediction formulas are recursions forwards in time (we use the solution at time $t_{n-1}$ to carry out the computation at time $t_{n}$).

* There are similar **backwards recursion formulas**,

<br>

[MP8] $\eqspace f_{Y_{n:N}|X_{n}}(\data{y_{n:N}}\given x_n)= f_{Y_n|X_n}(\data{y_n}\given x_n)
f_{Y_{n+1:N}|X_{n}}(\data{y_{n+1:N}}\given x_n)$.

<br>

[MP9] $\eqspace \displaystyle
f_{Y_{n+1:N}|X_{n}}(\data{y_{n+1:N}}\given x_n)$
<br>
$\eqspace\eqspace\displaystyle = \int f_{Y_{n+1:N}|X_{n+1}}(\data{y_{n+1:N}}\given x_{n+1}) \, 
f_{X_{n+1}|X_n}(x_{n+1}\given x_n)\, dx_n$.

<br>

* The forwards and backwards recursion formulas together allow us to compute the **smoothing formula**,

<br>

[MP10] $\eqspace f_{X_{n}|Y_{1:N}}(x_n\given \data{y_{1:N}})
\propto
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}}) \, 
f_{Y_{n:N}|X_{n}}(\data{y_{n:N}}\given x_n)$.

<br>

### Question: Show how [MP8], [MP9] and [MP10] follow from the basic properties of conditional densities combined with the Markov property. Find an expression for the constant of proportionality in [MP10].

<br>

------

-------

## Linear Gaussian POMP (LG-POMP) models

* Linear Gaussian partially observed Markov process (LG-POMP) models have many applications

    + Gassian ARMA models are LG-POMP models. The POMP recursion formulas give a computationally efficient way to obtain the likelihood of a Gaussian ARMA model. 

    + The computations for smoothing splines can be written as an LG-POMP model. This gives a route to computationally efficient spline smooothing.

    + The so-called **Basic Structural Model** is an LG-POMP used for econometric forecasting. It models a stochastic trend, seasonality, and measurement error, in a framework with econometrically interpretable parameters. By contrast, all but the simplest ARMA models are usually **black box** methods, which may fit the data but are not readily interpretable.

    + LG-POMP models are widely used in engineering, especially for control applications. 

    + In all scientific and engineering applications, if your situation is not too far from linear and Gaussian, you save a lot of effort if an LG-POMP model is appropriate. General nonlinear POMP models usually involve intensive Monte Carlo computation.

<br>

-------

------

### The general LG-POMP model

* Suppose the latent process, $X_{0:N}$, takes vector values with dimension $d_X$.

* Suppose the observation process, $\{Y_n\}$, takes vector values with dimension $d_Y$.

* A general mean zero LG-POMP model is specified by 

    + A sequence of $d_X\times d_X$ matrices, $\matA_{1:N}$,

    + A sequence of  $d_X\times d_X$ covariance matrices, $\covmatX_{0:N}$,

    + A sequence of $d_Y\times d_X$ matrices, $\matB_{1:N}$

    + A sequence of  $d_Y\times d_Y$ covariance matrices, $\covmatY_{1:N}$.

* We initialize with $X_0\sim N[0,\covmatX_0]$ and then define the entire LG-POMP model by a recursion for $n\in 1:N$,

<br>

[LG1] $\eqspace X_{n+1} = \matA_n X_n + \epsilon_n$, $\eqspace \epsilon_n\sim N[0,\covmatX_n]$,

<br>

[LG2] $\eqspace Y_{n} = \matB_n X_n + \eta_n$, $\eqspace \eta_n\sim N[0,\covmatY_n]$.

* Often, but not always, we will have a **time-homogeneous** LG-POMP model, with $\matA_n=\matA$, $\;\matB_n=\matB$, $\;\covmatX_n=\covmatX$ and $\covmatY_n=\covmatY$ for $n\in 1:N$.

<br>

--------

-------

### Example: the LG-POMP representation of a Gaussian ARMA

* Suppose $\{Y_n\}$ is a Gaussian ARMA(p,q) model with noise process $\omega_n\sim N[0,\sigma^2]$ and specification

<br>

[LG3] $\eqspace\displaystyle Y_n = \sum_{j=1}^p \ar_j Y_{n-j} +  \omega_n + \sum_{k=1}^q \ma_q \omega_{n-k}.$$

<br>

* Set $r=\max(p,q+1)$ so that $\{Y_n\}$ is also ARMA(r,r-1).

* Our LG-POMP representation has $d_X=r$, with
$$\matB_n = \matB = (1,0,0,\dots,0)$$
and 
$$\covmatY_n = \covmatY = 0.$$

* Therefore, $Y_n$ is the first component of $X_n$, observed without measurement error.

* Now, define
$$ X_n = \left(\begin{array}{l}
Y_n \\
\ar_2 Y_{n-1} + \dots + \ar_r Y_{n-r+1} + \ma_1 \omega_n + \dots +\ma_{r-1} \omega_{n-r+2}
\\
\ar_3 Y_{n-1} + \dots + \ar_r Y_{n-r+1} + \ma_2 \omega_n + \dots +\ma_{r-1} \omega_{n-r+3}
\\
\vdots
\\
\ar_r Y_{n-1} + \ma_{r-1} \omega_t
\end{array}\right)$$

* We can check that the ARMA equation [LG3] corresponds to the matrix equation
$$ X_{n+1} = \matA X_n + \left(\begin{array}{l}
1 \\
\ma_1\\
\ma_2\\
\vdots\\
\ma_{r-1}
\end{array}\right) \omega_n.
$$
where
$$
\matA = 
\left(\begin{array}{ccccc}
\ar_1 & 1 & 0 & \ldots & 0 \\
\ar_2 & 0 & 1 &       & 0 \\
\vdots & \vdots& \ddots & \\
\ar_{r-1} & 0 & & 1 \\
\ar_{r} & 0 \ldots & 0
\end{array}\right).$$
This is in the form of a time-homogenous LG-POMP, with $\matA$, $\matB$ and $\covmatY$ defined above, and
$$\covmatX_n = \covmatX = \sigma^2 (1, \ma_1,\ma_2,\dots,\ma_{r-1})^\transpose(1, \ma_1,\ma_2,\dots,\ma_{r-1}).$$

* There are other LG-POMP representations giving rise to the same ARMA model.

* When only one component of a latent process is observed, any model giving rise to the same observed component is indistinguishable from the data.

* Here, the LG-POMP model has order $r^2$ parameters and the ARMA model has order $r$ parameters, so we might expect there are many ways to parameterize the ARMA model as a special case of the much larger LG-POMP model.

<br>

--------

--------

### Example: The Basic Structural Model for econometrics

* The basic stuctural model supposes that the observation process $Y_{1:N}$ is the sum of a **level** ($L_n$),  a **trend** ($T_n$) describing the rate of change of the level, and a monthly **seasonal component** ($S_n$). The model supposes that all these quantities are perturbed with Gaussian white noise at each time point. So, we have the following model equations

<br>

$\begin{array}{lrcl}
\mbox{[BSM1]} \eqspace & Y_n &=& L_n + S_n + \epsilon_n \\
\mbox{[BSM2]} \eqspace & L_{n+1} &=& L_n + T_n + \xi_n \\
\mbox{[BSM3]} \eqspace & T_{n+1} &=& T_n + \zeta_n \\
\mbox{[BSM4]} \eqspace & S_{n+1} &=& -\sum_{k=0}^{10} S_{n-k} + \eta_n
\end{array}$

<br>

* We suppose $\epsilon_n\sim N[0,\sigma^2_\epsilon]$,  $\xi_n\sim N[0,\sigma^2_\xi]$,  $\zeta_n\sim N[0,\sigma^2_\zeta]$, and $\eta_n\sim N[0,\sigma^2_\eta]$.

* To complete the model, we need to specify initial values. There is no mention of this in the documentation of the  R implementation of the basic structural model, `StructTS`. We could go through the source code to find out what it does.

* Incidentally, `?StructTS` does give some advice which resonates with our experience earlier in the course that optimization for ARMA models is often imperfect.

     "Optimization of structural models is a lot harder than many of the
     references admit.  For example, the ‘AirPassengers’ data are
     considered in Brockwell & Davis (1996): their solution appears to
     be a local maximum, but nowhere near as good a fit as that
     produced by ‘StructTS’.  It is quite common to find fits with one
     or more variances zero, and this can include sigma^2_eps."

* To put [BSM1-4] in the form of an LG-POMP model, we set

<br>

[BSM5] $\eqspace X_n = (L_n,T_n,S_n, S_{n-1}, \dots,S_{n-10})^\transpose$.

<br>

Then, we have

<br>

[BSM6] $\eqspace Y_n = (1,0,1,0,0,\dots, 0) X_n + \epsilon_n$,

<br>

and 

[BSM7] $\eqspace \displaystyle
 \left(\begin{array}{l}
L_{n+1} \\
T_{n+1} \\
S_{n+1} \\
S_{n}\\
 \vdots \\
S_{n-9}
\end{array}\right)
=\matA
\left(\begin{array}{l}
L_n \\
T_n \\
S_n \\
S_{n-1}\\
 \vdots \\
S_{n-10}
\end{array}\right)
+
\left(\begin{array}{l}
\xi_n \\
\zeta_n \\
\eta_n \\
0 \\
 \vdots \\
0
\end{array}\right)$

<br>








